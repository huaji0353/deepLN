### 10-10
```
不同 语言的词标签 组合分布
v,n 动词名词

利用(nvn，dvv等)短语改进传统分词法
取消直接预测分词，而是逻辑组合

用 词的标签（v,n,d,u,a）动词，形容词+ 语调，来生成 句子

```

### 10-09
```
==猜想
做分词可能需要声调信息
字面上丢失了这样的信息
==猜想=

ですか？ - 吗？ - desuka?
ですが， - 不过, - desuga,

包含语调信息
不像中文一样，四个声调，一对多
==调查===
汉语 的音很长，而且组合起来更多种（汉语拼音字母表）声母，韵母，整体音节 23+24+16=（63个）
汉语拼音音节全表【长 又 稀疏】

日语 只有五十音（50个）清音+浊音+组合 那些
相比起来虽然短，但比 日语 多多的 英语 IPA短（国际音标表 48个）

===改进==
中日翻译
大量的汉字可以沿用，不能直译，或者含义，尽可能引用原句
「如果你正为生活所苦的话，要和我一起来吗？」
「もしも生活に苦労されているようでしたら、私と一緒に来ませんか？」
苦労比苦好

```

### 10-08
```
我最近读了点LN，发现，。符号断句这里
有很多信息啊

去掉，。混成一堆
然后人工断句，对比原文，全对

是不是能把断句融合成一块
就像大块程序逻辑流程换成一句单行去做
===
而且用换行替换，。
让文章更好读了

可能是现代人，对符号不敏感，寻址（读哪）代价很高
经常使用大量的短句与meme（网聊）
结合VN去解释：
停顿很难，我们需要短句
===
好像很难的样子
大多数前后句是形容，修饰，增强，前后句关联

拆开容易，融合难
在符号分句级别耦合,断句

写作本身就是句子有强耦合
句子与脑中场景分割用符号

文就是这样的东西

```
### 10-xx
```
日语 平假名 片假名 结合
与大量简易，连词，单词，都是平假名
而片假名做特有名词
其余都是汉字+平假名，导致特征极其明显

英文，通过变形单词，词性等等

日语表面特征最多>英文词性特征信息多>中文的表面信息量最少，冗余与单词关联非常弱，单句耦合弱

中文是表征最困难的语言（生成等任务）
华语的拟声词少，感情词少，所以极其难通过字面烘托【感情信息不足】

但通过映射其他语言，可以达到一定的感情？（还是语境？还是什么）对照
这就是为什么，外语比母语更有感情
====
生成模型使用，来控制维度，而不是map，分词等等

====

符号断句->断句关联（程序化/句法）->单句语法（句式，语法）->词汇（俚语，meme，变形）【这里需要字典而不是神经模型】
各种语言全部都有的通用性
```
